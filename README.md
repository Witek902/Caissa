# Caissa Chess Engine

[![LinuxBuildStatus](https://github.com/Witek902/Caissa/workflows/Linux/badge.svg)](https://github.com/Witek902/Caissa/actions/workflows/linux.yml)

![ArtImage](https://user-images.githubusercontent.com/5882734/193368109-abce432b-85e9-4f11-bb3c-57fd3d27db22.jpg?raw=true)
<p style='text-align: right;'><em>(image generated with DALLÂ·E 2)</em></p>

## Overview

Strong, UCI command-line chess engine, written from scratch in C++ in development since early 2021. Optimized for regular chess, FRC (Fischer Random Chess) and DFRC (Double Fischer Random Chess).

## Playing strength

Caissa is listed on many chess engines ranking lists:

* [CCRL 40/2 FRC](https://www.computerchess.org.uk/ccrl/404FRC/) - **3870** (#6) (version 1.14.1)
* [CCRL Chess324](https://www.computerchess.org.uk/ccrl/Chess324/rating_list_all.html) - **3639** (#6) (version 1.14)
* [CCRL 40/15](https://www.computerchess.org.uk/ccrl/4040/) - **3479** (#10) (version 1.13.1 4CPU)
* [CCRL Blitz](https://www.computerchess.org.uk/ccrl/404/) - **3682** (#7) (version 1.14.1 8CPU)
* [SPCC UHO-Top15](https://www.sp-cc.de) - **3630** (#8) (version 1.14)
* [IpMan Chess 10+1 (R9-7945HX)](https://ipmanchess.yolasite.com/r9-7945hx.php) - **3461** (#14) (version 1.14.1 avx512)
* [IpMan Chess 10+1 (i9-7980XE)](https://ipmanchess.yolasite.com/i9-7980xe.php) - **3386** (#20) (version 1.11 avx512)
* [IpMan Chess 5+0](https://ipmanchess.yolasite.com/i7-11800h.php) - **3381** (#28) (version 1.8)
* [CEGT 40/20](http://www.cegt.net/40_40%20Rating%20List/40_40%20SingleVersion/rangliste.html) - **3472** (#12) (version 1.13)
* [CEGT 40/4](http://www.cegt.net/40_4_Ratinglist/40_4_single/rangliste.html) - **3513** (#10) (version 1.13.1)
* [CEGT 5+3](http://www.cegt.net/5Plus3Rating/BestVersionsNEW/rangliste.html) - **3530** (#9) (version 1.13.1)
* [FGRL](http://www.fastgm.de/60-0.60.html) - **3253** (#18) (version 1.5)

## History / Originality

The engine has been written from the ground up. In early versions it used a simple PeSTO evaluation, which was replaced by the Stockfish NNUE for a short time. Since version 0.7, Caissa uses it's own efficiently updated neural network, trained with Caissa self-play games using a custom trainer. In a way, the first own Caissa network is based on Stockfish's network, but it was much weaker because of the small data set used back then (a few million positions). Currently (as of version 1.14) over 4.5 billion newly generated positions are used. Also, the old self-play games are successively purged, so that the newer networks are trained only on the most recent games generated by the most recent engine, and so on.

The runtime neural network evaluation code is located in [PackedNeuralNetwork.cpp](https://github.com/Witek902/Caissa/blob/devel/src/backend/PackedNeuralNetwork.cpp) and was inspired by [nnue.md document](https://github.com/glinscott/nnue-pytorch/blob/master/docs/nnue.md). The neural network trainer is written completely from scratch and is located in [NetworkTrainer.cpp](https://github.com/Witek902/Caissa/blob/devel/src/utils/NetworkTrainer.cpp), [NeuralNetwork.cpp](https://github.com/Witek902/Caissa/blob/devel/src/utils/NeuralNetwork.cpp) and other NeuralNetwork* files. The trainer is purely CPU-based and is heavily optimized to take advantage of many threads and AVX instructions as well as it exploits the sparse nature of the nets.

The games are generated with the utility [SelfPlay.cpp](https://github.com/Witek902/Caissa/blob/devel/src/utils/SelfPlay.cpp), which generates games with a fixed number of nodes/depth and saves them in a custom binary game format to save space. The opening books used are either Stefan's Pohl [UHO books](https://www.sp-cc.de/uho_2022.htm) or DFRC openings with few random moves played at the beginning.

### Supported UCI options

* **Hash** (int) Sets the size of the transposition table in megabytes.
* **MultiPV** (int) Sets the number of PV lines to search and print.
* **MoveOverhead** (int) Sets move overhead in milliseconds. Should be increased if the engine loses time.
* **Threads** (int) Sets the number of threads used for searching.
* **Ponder** (bool) Enables pondering.
* **EvalFile** (string) Neural network evaluation file.
* **EvalRandomization** (int) Allows introducing non-determinism and weakens the engine.
* **StaticContempt** (int) Static contempt value used throughout whole game.
* **DynamicContempt** (int) Dynamic contempt value used in the opening/middlegame stage.
* **SyzygyPath** (string) Semicolon-separated list of paths to Syzygy endgame tablebases.
* **SyzygyProbeLimit** (int) Maximum number of pieces on the board where Syzygy tablebases can be used.
* **UCI_AnalyseMode** (bool) Enables analysis mode: search full PV lines and disable any depth constraints.
* **UCI_Chess960** (bool) Enables chess 960 mode: castling moves are printed as "king captures rook".
* **UCI_ShowWDL** (bool) Print win/draw/loss probabilities along with classic centipawn evaluation.
* **UseSAN** (bool) Enables short algebraic notation output (FIDE standard) instead of default long algebraic notation.
* **ColorConsoleOutput** (bool) Enables colored console output for better readability.


### Provided EXE versions

* **AVX-512** - Fastest, requires a x64 CPU with AVX-512 instruction set support. May not be supported on consumer-grade CPUs.
* **BMI2** - Fast, requires a x64 CPU with AVX2 and BMI2 instruction set support. Supported by majority of modern CPUs.
* **AVX2** - Fast, requires a x64 CPU with AVX2 instruction set support. May be faster than BMI2 on some older CPUs (e.g. Intel Haswell processors).
* **POPCNT** - Slower, requires a x64 CPU with SSE4 and POPCNT instruction set support. For older CPUs.
* **Legacy** - Slowest, requires any x64 CPU. For very old x64 CPUs.

## Features

#### General
* UCI protocol
* Neural network evaluation
* Syzygy and Gaviota endgame tablebases support
* Chess960 (Fischer Random) support

#### Search Algorithm
* Negamax with alpha-beta pruning
* Iterative Deepening with Aspiration Windows
* Principal Variation Search (PVS)
* Quiescence Search
* Transposition Table
* Multi-PV search
* Multithreaded search via shared transposition table

#### Evaluation
* Neural network evaluation
  * (5x768&rarr;1024)x2&rarr;1 architecture
  * effectively updated first layer
  * manually vectorized code supporting SSE2, AVX2, AVX-512 and ARM NEON instructions
  * clipped-ReLU activation function
  * 8 variants of last layer weights selected based on piece count
  * input features: absolute piece coordinates with horizontal symmetry, 5 king buckets
* Special endgame evaluation routines

#### Neural net trainer
* Custom CPU-based trainer using Adam algorithm
* Heavily optimized using AVX instructions, multithreading, and exploiting sparsity of the first layer input
* Network trained on data generated purely from self-play games

#### Misc
* Large Pages Support for Transposition Table
* Magic Bitboards
* Handling non-standard chess positions (e.g. 64 pieces on the board, etc.)
* Outstanding performance at ultra-short games (sub-second for whole game).

## Modules

The projects comprises following modules:
  * _backend_ (library) - engine's core
  * _frontend_ (executable) - UCI wrapper for the backend
  * _utils_ (executable) - various utilities, such as unit tests, neural network trainer, self-play data generator, etc.


## Compilation

### Linux - makefile

To compile for Linux just call `make` in `src` directory:
```
cd src
make -j
```
**NOTE:** This will compile the default AVX2/BMI2 version.


### Linux - CMake

To compile for Linux using CMake:
```
mkdir build
cd build
cmake -DCMAKE_BUILD_TYPE=Final ..
make -j
```

**NOTE:** Currently, the project compiles with AVX2/BMI2 support by default.

There are three configurations supported:
* **Final** - final version, without asserts, etc.
* **Release** - development version with asserts enabled and with optimizations enabled for better performance
* **Debug** - development version with asserts enabled and optimizations disabled

### Windows - Visual Studio

To compile for Windows, use `GenerateVisualStudioSolution.bat` to generate Visual Studio solution. The only tested Visual Studio version is 2022. Using CMake directly in Visual Studio was not tested.

**NOTE:** After compilation make sure you copy appropriate neural net file from `data/neuralNets` directory to location where executable file is generated (`build/bin` on Linux or `build\bin\x64\<Configuration>` on Windows).
